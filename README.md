# joking-model

Проект выполнен в качестве домашнего задания по рекурентным нейронным сетям курса [Deep Learning 1](https://github.com/isadrtdinov/intro-to-dl-hse).  Данные которые использовались при обучении модели доступны по [ссылке](https://github.com/hzchet/joking-model/blob/main/jokes.txt.zip).

Данный репозиторий содержит код для генерации шуток с помощью языковой модели — это авторегрессионная вероятностная модель, которая предсказывает распределение следующего токена при условии предыдущих:
$$p(x_1, x_2, \ldots, x_T) = p(x_1)\cdot p(x_2 | x_1)\cdot p(x_3 | x_1, x_2)\cdot \ldots \cdot p(x_T | x_1, x_2, \ldots, x_{T-1})$$
Код поддерживает возможность работать как с оригинальной `RNN`, так и c `LSTM`. На каждом временном шаге модель возвращает логиты вероятностей для следующего токена. Модель работает в двух режимах:
- В режиме обучения (метод `forward`) модель принимает настоящие последовательности из датасета и их длины. На каждом временном шаге возвращаются логиты вероятностей следующего токена, что позволяет считать лосс, обучаться на трейне и валидироваться на валидации.
- В режиме генерации (инференса, метод `inference`) модель принимает некоторый префикс (возможно пустой), с которого начинать генерацию, и продолжает его. Для этого на каждом шаге генерируются новые логиты, семплируется новый токен (из распределения, заданного логитами), и процесс продолжается, пока не будет сгенерирован токен `UNK` или не будет достигнуто ограничение на длину последовательности. Чтобы получить больше контроля над генерацией, вводится параметр температуры `temp`. Перед семплированием логиты полученные из модели делятся на этот параметр `temp`.
